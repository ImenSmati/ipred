
R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

> library('ipred')
Loading required package: rpart
Loading required package: MASS
Loading required package: mlbench
Loading required package: survival
Loading required package: splines
Loading required package: nnet
Loading required package: class
> 
> actversion <- paste(R.version$major, R.version$minor, sep=".")
> thisversion <- "1.7.0"
> 
> #if (compareVersion(actversion, thisversion) >= 0) {
> #  RNGversion("1.6.2")
> #}
> set.seed(29081975)
> 
> 
> # Classification
> 
> learn <- as.data.frame(mlbench.twonorm(200))
> test <- as.data.frame(mlbench.twonorm(100))
> 
> # bagging
> 
> mod <- bagging(classes ~ ., data=learn, coob=TRUE, nbagg=10)
> mod

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = classes ~ ., data = learn, coob = TRUE, 
    nbagg = 10)

Out-of-bag estimate of misclassification error:  0.19 

> predict(mod)[1:10]
 [1] 1 1 1 1 2 2 1 1 1 1
Levels: 1 2
> 
> # Double-Bagging
> 
> comb.lda <- list(list(model=lda, predict=function(obj, newdata)
+                       predict(obj, newdata)$x))
> 
> mod <- bagging(classes ~ ., data=learn, comb=comb.lda, nbagg=10)
> mod

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = classes ~ ., data = learn, comb = comb.lda, 
    nbagg = 10)


> predict(mod, newdata=test[1:10,])
 [1] 1 1 1 1 2 2 1 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], agg="aver")
 [1] 1 1 1 1 2 2 1 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], agg="wei")
 [1] 1 1 1 1 1 2 1 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], type="prob")
        1   2
 [1,] 1.0 0.0
 [2,] 0.9 0.1
 [3,] 1.0 0.0
 [4,] 1.0 0.0
 [5,] 0.5 0.5
 [6,] 0.0 1.0
 [7,] 1.0 0.0
 [8,] 0.0 1.0
 [9,] 1.0 0.0
[10,] 0.0 1.0
> predict(mod, newdata=test[1:10,], type="prob", agg="aver")
        1   2
 [1,] 1.0 0.0
 [2,] 0.9 0.1
 [3,] 1.0 0.0
 [4,] 1.0 0.0
 [5,] 0.5 0.5
 [6,] 0.0 1.0
 [7,] 1.0 0.0
 [8,] 0.0 1.0
 [9,] 1.0 0.0
[10,] 0.0 1.0
> predict(mod, newdata=test[1:10,], type="prob", agg="wei")
              1           2
 [1,] 1.0000000 0.000000000
 [2,] 0.9976303 0.002369668
 [3,] 1.0000000 0.000000000
 [4,] 1.0000000 0.000000000
 [5,] 0.5311111 0.468888889
 [6,] 0.0000000 1.000000000
 [7,] 1.0000000 0.000000000
 [8,] 0.0000000 1.000000000
 [9,] 1.0000000 0.000000000
[10,] 0.0000000 1.000000000
> 
> mypredict.lda <- function(object, newdata)
+        predict(object, newdata = newdata)$class
> 
> errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda)

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.04 

> errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda,
+   est.para=control.errorest(k=5, random=FALSE))

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, est.para = control.errorest(k = 5, 
        random = FALSE))

	 5-fold cross-validation estimator of misclassification error 

Misclassification error:  0.04 

> 
> lapply(errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda,
+   est.para=control.errorest(k=5, random=FALSE, getmodels=TRUE))$models, class)
[[1]]
[1] "lda"

[[2]]
[1] "lda"

[[3]]
[1] "lda"

[[4]]
[1] "lda"

[[5]]
[1] "lda"

> errorest(classes ~ ., data=learn, model=bagging,
+          est.para=control.errorest(k=2), nbagg=10)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = bagging, 
    est.para = control.errorest(k = 2), nbagg = 10)

	 2-fold cross-validation estimator of misclassification error 

Misclassification error:  0.105 

> errorest(classes ~ ., data=learn, model=bagging,
+          est.para=control.errorest(k=2), nbagg=10, comb=comb.lda)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = bagging, 
    est.para = control.errorest(k = 2), nbagg = 10, comb = comb.lda)

	 2-fold cross-validation estimator of misclassification error 

Misclassification error:  0.045 

> errorest(classes ~ ., data=learn, model=lda,
+ predict=mypredict.lda, estimator="boot")

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, estimator = "boot")

	 Bootstrap estimator of misclassification error 
	 with 25 bootstrap replications

Misclassification error:  0.0395 
Standard deviation: 0.0034 

> errorest(classes ~ ., data=learn, model=lda,
+ predict=mypredict.lda, estimator="632plus")

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, estimator = "632plus")

	 .632+ Bootstrap estimator of misclassification error 
	 with 25 bootstrap replications

Misclassification error:  0.0326 

> 
> # Regression
> 
> learn <- as.data.frame(mlbench.friedman1(100))
> test <- as.data.frame(mlbench.friedman1(100))
> 
> # bagging
> 
> mod <- bagging(y ~ ., data=learn, coob=TRUE, nbagg=10)
> mod

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, coob = TRUE, 
    nbagg = 10)

Out-of-bag estimate of root mean squared error:  3.8853 

> predict(mod)[1:10]
 [1] 10.81664 17.73551 11.59625 11.12409 12.64024 11.54341       NA 13.37817
 [9] 21.40661       NA
> 
> predict(mod, newdata=test[1:10,])
 [1] 17.38947 19.63268 10.65900 20.19306 17.21303 10.61301 17.93208 11.61466
 [9] 10.07293 14.95946
> predict(mod, newdata=test[1:10,], agg="aver") 
 [1] 17.38947 19.63268 10.65900 20.19306 17.21303 10.61301 17.93208 11.61466
 [9] 10.07293 14.95946
> predict(mod, newdata=test[1:10,], agg="wei")  
 [1] 17.66740 19.65567 10.89005 20.17381 17.38077 10.77897 18.22338 11.62500
 [9] 10.13806 14.60351
> errorest(y ~ ., data=learn, model=lm)

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm)

	 10-fold cross-validation estimator of root mean squared error

Root mean squared error:  2.9591 

> errorest(y ~ ., data=learn, model=lm,
+          est.para=control.errorest(k=5, random=FALSE))

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm, 
    est.para = control.errorest(k = 5, random = FALSE))

	 5-fold cross-validation estimator of root mean squared error

Root mean squared error:  2.905 

> lapply(errorest(y ~ ., data=learn, model=lm,
+                 est.para=control.errorest(k=5, random=FALSE, getmodels=TRUE))$models, class)
[[1]]
[1] "lm"

[[2]]
[1] "lm"

[[3]]
[1] "lm"

[[4]]
[1] "lm"

[[5]]
[1] "lm"

> errorest(y ~ ., data=learn, model=lm, estimator="boot")

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm, 
    estimator = "boot")

	 Bootstrap estimator of root mean squared error 
	 with 25 bootstrap replications

Root mean squared error:  3.0737 

> 
> # survival
> 
> learn <- rsurv(100, model="C")
> test <- rsurv(100, model="C")
> 
> mod <- bagging(Surv(time, cens) ~ ., data=learn, nbagg=10)
> mod

Bagging survival trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    nbagg = 10)


> predict(mod, newdata=test[1:10,])
[[1]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
119.000 119.000   0.489   0.398   0.489 

[[2]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

       n   events   median  0.95LCL  0.95UCL 
142.0000 142.0000   0.0508   0.0435   0.0675 

[[3]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
122.000 122.000   0.489   0.398   0.532 

[[4]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

       n   events   median  0.95LCL  0.95UCL 
141.0000 141.0000   0.0378   0.0336   0.0435 

[[5]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

       n   events   median  0.95LCL  0.95UCL 
134.0000 134.0000   0.0359   0.0302   0.0435 

[[6]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
131.000 131.000   0.398   0.361   0.489 

[[7]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

       n   events   median  0.95LCL  0.95UCL 
131.0000 131.0000   0.0336   0.0197   0.0378 

[[8]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
127.000 127.000   0.398   0.361   0.489 

[[9]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

       n   events   median  0.95LCL  0.95UCL 
139.0000 139.0000   0.0187   0.0139   0.0302 

[[10]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

       n   events   median  0.95LCL  0.95UCL 
157.0000 157.0000   0.1050   0.0568   0.1375 

> 
> #errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
> #         est.para=list(k=2, random=FALSE), nbagg=5)
> #errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
> #         estimator="boot", nbagg=5, est.para=list(nboot=5))
> #insert control.errorest
> errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          est.para=control.errorest(k=2, random=FALSE), nbagg=5)

Call:
errorest.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    model = bagging, est.para = control.errorest(k = 2, random = FALSE), 
    nbagg = 5)

	 2-fold cross-validation estimator of Brier's score

Brier's score:  0.0953 

> errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          estimator="boot", nbagg=5, est.para=control.errorest(nboot=5))

Call:
errorest.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    model = bagging, estimator = "boot", est.para = control.errorest(nboot = 5), 
    nbagg = 5)

	 Bootstrap estimator of Brier's score
	 with 5 bootstrap replications

Brier's score:  0.0947 

> 
> #lapply(errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
> #         estimator="cv", nbagg=1, est.para=list(k=2, random=FALSE,
> #         getmodels=TRUE))$models, class)
> #insert control.errorest
> lapply(errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          estimator="cv", nbagg=1, est.para=control.errorest(k=2, random=FALSE,
+          getmodels=TRUE))$models, class)
[[1]]
[1] "survbagg"

[[2]]
[1] "survbagg"

> 
> # bundling for regression
> 
> learn <- as.data.frame(mlbench.friedman1(100))
> test <- as.data.frame(mlbench.friedman1(100))
> 
> comb <- list(list(model=lm, predict=predict.lm))
> 
> modc <- bagging(y ~ ., data=learn, nbagg=10, comb=comb)
> modc

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, nbagg = 10, 
    comb = comb)


> predict(modc, newdata=learn)[1:10]
 [1] 21.880856  7.246768 15.482136 11.982010 15.758885 13.131722  7.039301
 [8] 21.829846  7.785673 14.375108
> 
> # bundling for survival
> 
> data(GBSG2)
> rcomb <- list(list(model=coxph, predict=predict))
> 
> mods <- bagging(Surv(time,cens) ~ ., data=GBSG2, nbagg=10, 
+                 comb=rcomb,  control=rpart.control(xval=0))
> predict(mods, newdata=GBSG2[1:3,])
[[1]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
    903     443    1641    1528    1753 

[[2]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
    663     303    1753    1528    1814 

[[3]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

      n  events  median 0.95LCL 0.95UCL 
    699     411    1105    1002    1253 

> 
> # test for method dispatch on integer valued responses
> y <- sample(1:100, 100)
> class(y)
[1] "integer"
> x <- matrix(rnorm(100*5), ncol=5)
> mydata <- as.data.frame(cbind(y, x))
> 
> cv(y, y ~ ., data=mydata, model=lm, predict=predict)

	 10-fold cross-validation estimator of root mean squared error

Root mean squared error:  30.5108 

> bootest(y, y ~ ., data=mydata, model=lm, predict=predict)

	 Bootstrap estimator of root mean squared error 
	 with 25 bootstrap replications

Root mean squared error:  30.7493 

> bagging(y ~., data=mydata, nbagg=10)

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = mydata, nbagg = 10)


> 
