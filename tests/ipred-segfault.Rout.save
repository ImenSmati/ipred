
R : Copyright 2003, The R Development Core Team
Version 1.6.2  (2003-01-10)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

> library('ipred')
Loading required package: rpart 
Loading required package: MASS 
Loading required package: mlbench 
Loading required package: survival 
Loading required package: class 
Loading required package: nnet 
> 
> actversion <- paste(R.version$major, R.version$minor, sep=".")
> thisversion <- "1.7.0"
> 
> if (compareVersion(actversion, thisversion) >= 0) {
+   RNGversion("1.6.2")
+ }
> set.seed(29081975)
> 
> 
> # Classification
> 
> learn <- as.data.frame(mlbench.twonorm(200))
> test <- as.data.frame(mlbench.twonorm(100))
> 
> # bagging
> 
> mod <- bagging(classes ~ ., data=learn, coob=TRUE, nbagg=10)
> mod

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = classes ~ ., data = learn, coob = TRUE, 
    nbagg = 10)

Out-of-bag estimate of misclassification error:  0.17 

> predict(mod)[1:10]
 [1] 1 1 1 2 2 1 1 1 1 1
Levels: 1 2
> 
> # Double-Bagging
> 
> comb.lda <- list(list(model=lda, predict=function(obj, newdata)
+                       predict.lda(obj, newdata)$x))
> 
> mod <- bagging(classes ~ ., data=learn, comb=comb.lda, nbagg=10)
> mod

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = classes ~ ., data = learn, comb = comb.lda, 
    nbagg = 10)


> predict(mod, newdata=test[1:10,])
 [1] 1 2 1 2 2 2 2 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], agg="aver")
 [1] 1 2 1 2 2 2 2 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], agg="wei")
 [1] 1 2 1 2 2 2 2 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], type="prob")
        1   2
 [1,] 0.8 0.2
 [2,] 0.0 1.0
 [3,] 1.0 0.0
 [4,] 0.0 1.0
 [5,] 0.0 1.0
 [6,] 0.0 1.0
 [7,] 0.1 0.9
 [8,] 0.3 0.7
 [9,] 1.0 0.0
[10,] 0.0 1.0
> predict(mod, newdata=test[1:10,], type="prob", agg="aver")
        1   2
 [1,] 0.8 0.2
 [2,] 0.0 1.0
 [3,] 1.0 0.0
 [4,] 0.0 1.0
 [5,] 0.0 1.0
 [6,] 0.0 1.0
 [7,] 0.1 0.9
 [8,] 0.3 0.7
 [9,] 1.0 0.0
[10,] 0.0 1.0
> predict(mod, newdata=test[1:10,], type="prob", agg="wei")
               1           2
 [1,] 0.99333333 0.006666667
 [2,] 0.00000000 1.000000000
 [3,] 1.00000000 0.000000000
 [4,] 0.00000000 1.000000000
 [5,] 0.00000000 1.000000000
 [6,] 0.00000000 1.000000000
 [7,] 0.10081466 0.899185336
 [8,] 0.01419878 0.985801217
 [9,] 1.00000000 0.000000000
[10,] 0.00000000 1.000000000
> 
> mypredict.lda <- function(object, newdata)
+        predict(object, newdata = newdata)$class
> 
> errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda)

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.03 

> errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda,
+ est.para=list(k=5, random=FALSE))

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, est.para = list(k = 5, random = FALSE))

	 5-fold cross-validation estimator of misclassification error 

Misclassification error:  0.03 

> errorest(classes ~ ., data=learn, model=bagging, est.para=list(k=2),
+ nbagg=10)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = bagging, 
    est.para = list(k = 2), nbagg = 10)

	 2-fold cross-validation estimator of misclassification error 

Misclassification error:  0.155 

> errorest(classes ~ ., data=learn, model=bagging, est.para=list(k=2),
+ nbagg=10, comb=comb.lda)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = bagging, 
    est.para = list(k = 2), nbagg = 10, comb = comb.lda)

	 2-fold cross-validation estimator of misclassification error 

Misclassification error:  0.06 

> errorest(classes ~ ., data=learn, model=lda,
+ predict=mypredict.lda, estimator="boot")

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, estimator = "boot")

	 Bootstrap estimator of misclassification error 
	 with 25 bootstrap replications

Misclassification error:  0.048 
Standard deviation: 0.0052 

> errorest(classes ~ ., data=learn, model=lda,
+ predict=mypredict.lda, estimator="632plus")

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, estimator = "632plus")

	 .632+ Bootstrap estimator of misclassification error 
	 with 25 bootstrap replications

Misclassification error:  0.033 

> 
> # Regression
> 
> learn <- as.data.frame(mlbench.friedman1(100))
> test <- as.data.frame(mlbench.friedman1(100))
> 
> # bagging
> 
> mod <- bagging(y ~ ., data=learn, coob=TRUE, nbagg=10)
> mod

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, coob = TRUE, 
    nbagg = 10)

Out-of-bag estimate of root mean squared error:  3.1874 

> predict(mod)[1:10]
 [1]        NA 15.532137  7.306890  8.981709 14.445341 10.741602 17.357103
 [8] 11.093114 11.400889  7.545898
> 
> predict(mod, newdata=test[1:10,])
 [1] 19.477639 19.225918 16.964340  7.706251 14.497439 16.291955 13.541693
 [8]  9.843498 18.893207 19.511636
> predict(mod, newdata=test[1:10,], agg="aver") 
 [1] 19.477639 19.225918 16.964340  7.706251 14.497439 16.291955 13.541693
 [8]  9.843498 18.893207 19.511636
> predict(mod, newdata=test[1:10,], agg="wei")  
 [1] 19.795115 19.329776 17.186216  7.817882 14.448255 16.075408 13.607599
 [8] 10.104368 18.992186 19.495389
> errorest(y ~ ., data=learn, model=lm)

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm)

	 10-fold cross-validation estimator of root mean squared error

Root mean squared error:  2.7929 

> errorest(y ~ ., data=learn, model=lm, est.para=list(k=5, random=FALSE))

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm, 
    est.para = list(k = 5, random = FALSE))

	 5-fold cross-validation estimator of root mean squared error

Root mean squared error:  2.8395 

> errorest(y ~ ., data=learn, model=lm, estimator="boot")

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm, 
    estimator = "boot")

	 Bootstrap estimator of root mean squared error 
	 with 25 bootstrap replications

Root mean squared error:  2.926 

> 
> # survival
> 
> learn <- rsurv(100, model="C")
> test <- rsurv(100, model="C")
> 
> mod <- bagging(Surv(time, cens) ~ ., data=learn, nbagg=10)
> mod

Bagging survival trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    nbagg = 10)


> predict(mod, newdata=test[1:10,])
[[1]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 133.0000  133.0000    0.4573    0.0361    0.3211    0.2651    0.3811 

[[2]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 1.44e+02  1.44e+02  2.21e-02  2.29e-03  1.68e-02  1.18e-02  2.15e-02 

[[3]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 1.40e+02  1.40e+02  4.10e-02  4.36e-03  2.74e-02  2.45e-02  3.55e-02 

[[4]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 146.0000  146.0000    0.7953    0.0457    0.6637    0.5685    0.8553 

[[5]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 1.54e+02  1.54e+02  3.08e-02  2.87e-03  2.15e-02  1.98e-02  2.45e-02 

[[6]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 137.0000  137.0000    0.3398    0.0198    0.3152    0.2385    0.3811 

[[7]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 127.0000  127.0000    0.1428    0.0181    0.0474    0.0325    0.1068 

[[8]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 1.44e+02  1.44e+02  2.21e-02  2.29e-03  1.68e-02  1.18e-02  2.15e-02 

[[9]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 127.0000  127.0000    0.2356    0.0197    0.1701    0.1589    0.2188 

[[10]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
 117.0000  117.0000    0.1340    0.0125    0.1042    0.0548    0.1589 

> 
> errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          est.para=list(k=2, random=FALSE), nbagg=5)

Call:
errorest.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    model = bagging, est.para = list(k = 2, random = FALSE), 
    nbagg = 5)

	 2-fold cross-validation estimator of Brier's score

Brier's score:  0.0765 

> errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          estimator="boot", nbagg=5, est.para=list(nboot=5))

Call:
errorest.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    model = bagging, estimator = "boot", est.para = list(nboot = 5), 
    nbagg = 5)

	 Bootstrap estimator of Brier's score
	 with 5 bootstrap replications

Brier's score:  0.0721 

> 
> # bundling for regression
> 
> learn <- as.data.frame(mlbench.friedman1(100))
> test <- as.data.frame(mlbench.friedman1(100))
> 
> comb <- list(list(model=lm, predict=predict.lm))
> 
> modc <- bagging(y ~ ., data=learn, nbagg=10, comb=comb)
> modc

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, nbagg = 10, 
    comb = comb)


> predict(modc, newdata=learn)[1:10]
 [1] 22.136593  5.802498 19.063319 15.700646 18.800151 11.746587  9.941371
 [8] 14.538651 10.786794 11.811248
> 
> # bundling for survival
> 
> data(GBSG2)
> rcomb <- list(list(model=coxph, predict=predict.coxph))
> 
> mods <- bagging(Surv(time,cens) ~ ., data=GBSG2, nbagg=10, 
+                 comb=rcomb,  control=rpart.control(xval=0))
> predict(mods, newdata=GBSG2[1:3,])
[[1]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
    892.0     448.0    1577.9      30.7    1601.0    1502.0    1753.0 

[[2]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
      921       304      1879        32      2093      2018      2372 

[[3]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]))

        n    events     rmean se(rmean)    median   0.95LCL   0.95UCL 
    580.0     352.0    1270.3      36.2     918.0     859.0    1090.0 

> 
